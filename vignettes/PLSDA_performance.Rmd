---
title: "PLS-DA performance"
output: html_document
date: "2025-08-01"
---

# Introduction

This page presents an application of the PLSDA performance assessment. The PLS method is a quite particular method : there are several predictions according to the components number selected in the model. It is the same with PLSDA. The goal is almost to choose the best number of component in PLS regression in order to compute the best possible predictions. For that, we will use two datasets: 

- one is a dataset with only ten predictor variables $X = (X1,X2,...,X10)$ and two classes.

- the other is a dataset with forty predictor variables $X = (X1,X2,...,X40)$ and three classes. With $p = 40 > n = 30$, this dataset approches realist conditions for PLS training.

To access to predefined functions from sgPLSdevelop package and manipulate these datasets, run these lines :

```{r data, message=FALSE, warning=FALSE}
library(sgPLSdevelop)

data1 <- data.cl.create(p = 10, list = TRUE) # 2 classes by default
data2 <- data.cl.create(n = 30, p = 40, classes = 3, list = TRUE)
```

Now, it's time to train a PLS-DA model for each dataset built. 

```{r pls, message=FALSE, warning=FALSE}
ncomp.max <- 5

# First model
X <- data1$X
Y <- as.factor(data1$Y)
model1 <- PLSda(X,Y, ncomp = ncomp.max)

# Second model
X <- data2$X
Y <- as.factor(data2$Y)
model2 <- PLSda(X,Y, ncomp = ncomp.max)
```

In the continuation of this article, we will show PLS-DA performance assessment with mean error rate by using leave-one-out cross-validation (LOOCV), 10-fold CV and 5-fold CV. The `perf.PLSda` function will allow to compute the error rate for each application case. 

NB : there are three possible distances for computing the error rate : $\textit{maximum}$ distance, $\textit{centroÃ¯ds}$ distance and $\textit{Mahalanobis}$ distance. By default, this function uses $\textit{maximum}$ distance. In the most complicated cases, it is advisable to choose $\textit{Mahalanobis}$ distance which gives more accurate results.

# Leave-one-out CV

The Leave-one-out CV (LOOCV) builts $n$ models with a test set composed of only a single row (never the same row for each model). 

### First model 

Let's start with the first model.

```{r, fig.cap= "Error rate of the first model by LOOCV"}
perf.res1 <- perf.PLSda(model1)
h.best <- perf.res1$h.best
msep.best <- perf.res1$MSEP[h.best]
```

The `perf.PLS` function gives us a optimal components number equal to $H =$ `r h.best`, therefore we suggest to select `r h.best` component(s) in our first model.

### Second model

Let's continue with the second model.

```{r, fig.cap= "Error rate of the second model by LOOCV"}
perf.res2 <- perf.PLSda(model2)
h.best <- perf.res2$h.best
msep.best <- perf.res2$MSEP[h.best]
```

The `perf.PLS` function gives us a optimal components number equal to $H =$ `r h.best`, therefore we suggest to select `r h.best` component(s) in our first model.

The LOOCV is an efficient way to assess performance but requires a large computing capacity. The K-fold CV (which create K blocks) reduces not only the number of models to built but also the execution time.

# 10-fold CV

The 10-fold CV builts 10 models. In our case, for each model, the test set is composed of 4 observations for the first model and 3 for the second.

### First model 

```{r, fig.cap= "Error rate of the first model by 10-fold CV"}
perf.res1 <- perf.PLSda(model1, K = 10)
h.best <- perf.res1$h.best
msep.best <- perf.res1$MSEP[h.best]
```

The `perf.PLS` function gives us a optimal components number equal to $H =$ `r h.best`, therefore we suggest to select `r h.best` component(s) in our first model.

### Second model 

```{r, fig.cap= "Error rate of the second model by 10-fold CV"}
perf.res2 <- perf.PLSda(model2, K = 10)
h.best <- perf.res2$h.best
msep.best <- perf.res2$MSEP[h.best]
```

The `perf.PLS` function gives us a optimal components number equal to $H =$ `r h.best`, therefore we suggest to select `r h.best` component(s) in our first model.

# 5-fold CV

The 5-fold CV builts 5 models. In our case, for each model, the test set is composed of 8 observations for the first model and 6 for the second.

### First model 

```{r, fig.cap= "Error rate of the first model by 5-fold CV"}
perf.res1 <- perf.PLSda(model1, K = 5)
h.best <- perf.res1$h.best
msep.best <- perf.res1$MSEP[h.best]
```

The `perf.PLS` function gives us a optimal components number equal to $H =$ `r h.best`, therefore we suggest to select `r h.best` component(s) in our first model.

### Second model 

```{r, fig.cap= "Error rate of the second model by 5-fold CV"}
perf.res2 <- perf.PLSda(model2, K = 5)
h.best <- perf.res2$h.best
msep.best <- perf.res2$MSEP[h.best]
```

The `perf.PLS` function gives us a optimal components number equal to $H =$ `r h.best`, therefore we suggest to select `r h.best` component(s) in our first model.
